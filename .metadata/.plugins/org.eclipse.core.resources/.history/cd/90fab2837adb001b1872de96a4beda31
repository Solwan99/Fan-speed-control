/*
 * machinealgorithm.c
 *
 *  Created on: 2 Jul 2021
 *      Author: {Rana Mohamed}
 */
#define false 0
#define true  1
float ALPHA = 0.1;    /*LEARNING RATE*/
float GAMMA = 0.5;    /*DISCOUNT FACTOR*/


int Current_STATE;    /* CURRENT STATE OF THE ROBOT */

int States_count =8;
int Actions_count=4;
int ACTION_TAKEN = false;
int NEXT_STATE;
float Q_new,Q_old,Q_Max;

/*ACTION PERFORMED BY THE ROBOT(0:FORWARD, 1:right , 2:left, 3:stop)*/
int actions[]={0,1,2,3};

float Q[States_count][Actions_count]={{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0}};
int REWARDS[States_count][Actions_count]={{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0},{0,0,0,0}};

void Update(float Q_TABLE[][4] , int current_s, int next_s, int A, int ACTIONS[], int R, float LEARNING_RATE, float DISCOUNT_FACTOR)
{
  /*THIS FUNCTION UPDATES THE Q TABLE AND Q VALUES. THIS UPDATE KEEPS ON HAPPENING UNTILL THE
  MAIN LOOP ENDS. AT THE END OF EPISODES THE Q TABLE IS FILLED WITH VARIOUS VALUES. THE GREATER
  THE VALUES THE GREATER IMPORTANCE THE ACTION HAS AT THAT PARTICULAR STATE. "Q_OLD" IS OLD VALUE
  THAT THE Q MATRIX HAS.THIS IS THE VALUE WHICH GETS UPDATED EVENTUALLY. Q_NEW IS THE NEW Q_VALUE
  WHICH IS CALCULATED BY THE Q LEARNING FORMULA. THE Q LEARNING FORMULA USED HERE IS BASED ON
  BELLMAN EQUATION USES TEMPORAL DIFFERENCE LEARNING APPROACH.(MONTE CARLO APPROACH WILL NOT
  WORK IN THIS CASE OF OBSTACLE AVOIDING ROBOT.*/

  Q_old = Q_TABLE[current_s][A];
  Q_Max = MAX(Q_TABLE, next_s);
  Q_new = (1-LEARNING_RATE)*Q_old + LEARNING_RATE*(R + DISCOUNT_FACTOR * Q_Max);
  Q_TABLE[current_s][A] = Q_new;
}

// Robot's functions
void forward()
{

	}
void left()
{}
void right()
{}
void stop()
{}

// training

void training(){
 for (int i =0 ; i<episodes; i++)
 { int action;
   action = rand(0,4);
   if()

 }

}
int main(){

}
